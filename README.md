# CLARION-SUM  
### A Hierarchical Fact-Aware Self-Refining Framework for Multi-Document Clinical News Summarization

---

## ğŸ“Œ Overview

CLARION-SUM is a novel hierarchical, fact-aware, self-refining framework designed for **multi-document clinical news summarization**.

It integrates:

- Salience-driven graph modeling  
- Hierarchical transformer encoding  
- Long-context generation  
- Factual verification head  
- Iterative self-refinement loop  

The model aims to generate summaries that are:
- Factually consistent  
- Less redundant  
- Coherent  
- Clinically reliable  

---

## ğŸ—ï¸ System Architecture

CLARION-SUM consists of five major components:

1. **Sentence Segmentation**
2. **Salience Graph Module**
3. **Hierarchical Transformer Encoder**
4. **Long-Context Generator**
5. **Factual Verification + Self-Refinement Loop**

The refinement continues until factual consistency stabilizes.

---

## ğŸ§  Mathematical Formulation

Given a set of clinical documents:

D = {dâ‚, dâ‚‚, ..., dâ‚™}

We generate a summary:

S = {sâ‚, sâ‚‚, ..., sâ‚˜}

Optimization objective:

max Î±Rel(S, D) âˆ’ Î²Red(S) + Î³Fact(S, D) + Î´Coh(S)

Where:

- Rel(S, D) â†’ Semantic relevance  
- Red(S) â†’ Redundancy penalty  
- Fact(S, D) â†’ Factual consistency  
- Coh(S) â†’ Coherence  

---

## ğŸ¯ Training Objective

Total Loss:

L = L_gen + Î»â‚L_fact + Î»â‚‚L_red + Î»â‚ƒL_refine

- L_gen â†’ Cross-entropy generation loss  
- L_fact â†’ Factual inconsistency penalty  
- L_red â†’ Redundancy penalty  
- L_refine â†’ Iterative refinement improvement  

---

## ğŸ“Š Datasets

We evaluate CLARION-SUM on:

- **MEDIQA-ClinicalSumm**
- **PubMed**

Both datasets contain long clinical and biomedical documents suitable for multi-document summarization.

---

## ğŸ“ˆ Baselines (10 Models Compared)

### Extractive:
- Lead-3  
- TextRank  
- BERTSUM  

### Abstractive:
- BART  
- T5  
- PEGASUS  

### Long-document:
- Long-T5  
- LED  

### Hierarchical:
- HIBERT  

### LLM-based:
- Prompt-based LLM summarization  

---

## ğŸ“ Evaluation Metrics

- ROUGE-L  
- BERTScore  
- FactScore  
- Redundancy Score  

---

## ğŸ”¬ Key Contributions

âœ” Salience-aware graph-based importance modeling  
âœ” Integrated factual verification during training  
âœ” Iterative self-refinement mechanism  
âœ” 10-model comparative evaluation  
âœ” Mathematical formulation + ablation study  

---

## âš™ï¸ Implementation Details

- Framework: PyTorch  
- Library: HuggingFace Transformers  
- Optimizer: AdamW  
- Learning Rate: 3e-5  
- Batch Size: 4  
- GPU: NVIDIA GPU (16GB)  

---

## ğŸš€ Future Work

- Larger-scale clinical datasets  
- Stronger factual alignment mechanisms  
- Clinical domain-specific pretraining  

---

## ğŸ“œ License

For research and academic purposes only.

---

## ğŸ‘¤ Author

Shekhar  
Research Intern â€“ Suvidha Foundation NGO  

---

â­ If you find this work useful, consider starring the repository!
